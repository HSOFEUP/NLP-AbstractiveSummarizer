# NLP-AbstractiveSummarizer
Text summarization is a very challenging problem that can only be truly realized by understanding meaning of the textual content. Recently, deep recurrent neural networks have been used in the sequence to sequence framework to achieve good results on summarization tasks. In this project we explore the success achieved by a stacked LSTM encoder â€“ attention based decoder architecture on the English Gigaword dataset. We employ the usual best practices of sequence to sequence models with a complete end-to-end training, and have decent results to show on generating summaries of 10 words or less, for 2 line texts with a maximum of 30 words. We also explore on improvements in network training time, computation and refinement of summaries through various experiments.
